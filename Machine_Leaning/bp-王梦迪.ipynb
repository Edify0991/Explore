{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8842daf",
   "metadata": {},
   "source": [
    "BP神经网络\n",
    "\n",
    "根据BP神经网络原理，实现简单的BP神经网络(64, 100, 10)，并绘制网络结构简图和标注\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cca6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import LabelBinarizer# 标签二值化\n",
    "from sklearn.model_selection import train_test_split # 切割数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32196cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()# 载入数据\n",
    "X = digits.data# 数据\n",
    "y = digits.target# 标签\n",
    "\n",
    "# 数据归一化\n",
    "X -= X.min()\n",
    "X /= X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda6b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):                                                                   \n",
    "    ret = np.zeros((len(x)))                                                      \n",
    "    for i in range(len(x)):                                                       \n",
    "        if x[i] >= 0:                                                             \n",
    "            ret[i] = 1.0 / (1 + np.exp(-x[i]))                                    \n",
    "        else:                                                                     \n",
    "            ret[i] = np.exp(x[i]) / (1 + np.exp(x[i]))                            \n",
    "    return ret                                                                    \n",
    "                                                                                  \n",
    "def dsigmoid(x):                                                                  \n",
    "    #print(x)                                                                     \n",
    "    return x * (1 - x)                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe50a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:                                                                                                                                                                                                \n",
    "    # 初始化网络                                                                                                                                                                                                         \n",
    "    def __init__(self, layers):  # (64,100,10)                                                                                                                                                                      \n",
    "        self.layers_num = len(layers)   #神经网络层数                                                                                                                                                                     \n",
    "        self.layers = layers                                                                                                                                                                                        \n",
    "        #除去输入层，其余层需要随机产生n个神经元的bias值，在（0，1）之间                                                                                                                                                                        \n",
    "        self.biases = [np.random.randn(y, 1) for y in layers[1: ]]  #randn的参数表示产生随机数数量的各维度大小，layers[1:]其中的1是因为从第二层开始才需要bias                                                                                         \n",
    "        #随机产生每条神经元连接线的权重值，在（0，1）之间                                                                                                                                                                                  \n",
    "        #zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。                                                                                                                                                    \n",
    "        #如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表                                                                                                                                                       \n",
    "        #如a = [1,2,3] b = [4,5,6] zipped = zip(a,b) ： [(1, 4), (2, 5), (3, 6)]                                                                                                                                      \n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(layers[: -1], layers[1: ])]                                                                                                                           \n",
    "                                                                                                                                                                                                                    \n",
    "    def cost_derivative(self, output_activations, y):                                                                                                                                                               \n",
    "        return (output_activations - y)                                                                                                                                                                             \n",
    "                                                                                                                                                                                                                    \n",
    "    def backprop(self, x, y):                                                                                                                                                                                       \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]                                                                                                                                                          \n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]                                                                                                                                                         \n",
    "        # 前向传输                                                                                                                                                                                                      \n",
    "        #每个数据是一个长64的array数组                                                                                                                                                                                         \n",
    "        activation = x                                                                                                                                                                                              \n",
    "        # 储存每层的神经元的值的矩阵，下面循环会 append 每层的神经元的值                                                                                                                                                                       \n",
    "        activations = [x]                                                                                                                                                                                           \n",
    "        # 储存每个未经过 sigmoid 计算的神经元的值                                                                                                                                                                                  \n",
    "        zs = []                                                                                                                                                                                                     \n",
    "        #print(self.biases)                                                                                                                                                                                         \n",
    "        for b, w in zip(self.biases, self.weights):                                                                                                                                                                 \n",
    "            #print(activation.ndim)                                                                                                                                                                                 \n",
    "            if b.ndim == 1:                                                                                                                                                                                         \n",
    "                b = b.reshape(1, len(b))                                                                                                                                                                            \n",
    "            if activation.ndim == 1:                                                                                                                                                                                \n",
    "                activation = activation.reshape(1, len(activation))                                                                                                                                                 \n",
    "            if w.shape[1] != activation.shape[0]:                                                                                                                                                                   \n",
    "                activation = activation.transpose()                                                                                                                                                                 \n",
    "            dot_res = np.dot(w, activation)                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                    \n",
    "            z = dot_res + b                                                                                                                                                                                         \n",
    "            #print(z)                                                                                                                                                                                               \n",
    "            #print((np.dot(w, activation)).shape)                                                                                                                                                                   \n",
    "            #print(w.shape)                                                                                                                                                                                         \n",
    "            #print(z.shape)                                                                                                                                                                                         \n",
    "            zs.append(z)                                                                                                                                                                                            \n",
    "            activation = sigmoid(z)                                                                                                                                                                                 \n",
    "            #print(activation.shape)                                                                                                                                                                                \n",
    "            activations.append(activation)                                                                                                                                                                          \n",
    "                                                                                                                                                                                                                    \n",
    "                                                                                                                                                                                                                    \n",
    "        # 求 δ 的值                                                                                                                                                                                                    \n",
    "        #print(zs[-1])                                                                                                                                                                                              \n",
    "        delta = self.cost_derivative(activations[-1].reshape(-1), y) * dsigmoid(activations[-1].reshape(-1))                                                                                                        \n",
    "        delta = delta.reshape(len(delta), 1)                                                                                                                                                                        \n",
    "        nabla_b[-1] = delta                                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                    \n",
    "        nabla_w[-1] = np.dot(delta.reshape(1, len(delta)).transpose(), activations[-2].reshape(1, len(activations[-2])))                                                                                            \n",
    "                                                                                                                                                                                                                    \n",
    "        #这里进行逆向遍历，比如当l为2时，zs[-l]为网络的倒数第二层                                                                                                                                                                           \n",
    "        for l in range(2, self.layers_num):                                                                                                                                                                         \n",
    "            activation = activations[-l]                                                                                                                                                                            \n",
    "            sp = dsigmoid(activation)                                                                                                                                                                               \n",
    "                                                                                                                                                                                                                    \n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp.reshape(len(sp), 1)                                                                                                                        \n",
    "                                                                                                                                                                                                                    \n",
    "            nabla_b[-l] = delta                                                                                                                                                                                     \n",
    "                                                                                                                                                                                                                    \n",
    "            nabla_w[-l] = np.dot(delta.reshape(len(delta), 1), activations[-l - 1].reshape(1, len(activations[-l - 1])))                                                                                            \n",
    "        #print(nabla_w)                                                                                                                                                                                             \n",
    "        return (nabla_b, nabla_w)                                                                                                                                                                                   \n",
    "    def train(self, x_data, y_data, lr=0.3, epochs=10000):                                                                                                                                                          \n",
    "        accuracy = []  # 用来保存测试过程中的准确率                                                                                                                                                                              \n",
    "        loss = []  # 用来保存测试时产生的代价函数的值                                                                                                                                                                               \n",
    "        for n in range(epochs + 1):                                                                                                                                                                                 \n",
    "            # 更新权重                                                                                                                                                                                                  \n",
    "            # 根据 biases 和 weights 的行列数创建对应的全部元素值为 0 的空矩阵                                                                                                                                                            \n",
    "            nabla_b = [np.zeros(b.shape) for b in self.biases]                                                                                                                                                      \n",
    "            nabla_w = [np.zeros(w.shape) for w in self.weights]                                                                                                                                                     \n",
    "            for i in range(len(x_data)):                                                                                                                                                                            \n",
    "                # 根据样本中的每一个输入 x 的其输出 y，计算 w 和 b 的偏导数                                                                                                                                                                \n",
    "                x = x_data[i]                                                                                                                                                                                       \n",
    "                y = y_data[i]                                                                                                                                                                                       \n",
    "                #print(y)                                                                                                                                                                                           \n",
    "                delta_nabla_b, delta_nabla_w = self.backprop(x, y)                                                                                                                                                  \n",
    "                #print(delta_nabla_b)                                                                                                                                                                               \n",
    "                # 累加储存偏导值 delta_nabla_b 和 delta_nabla_w                                                                                                                                                             \n",
    "                #nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]                                                                                                                                    \n",
    "                #nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]                                                                                                                                    \n",
    "                self.weights = [w - lr * nw for w, nw in zip(self.weights, delta_nabla_w)]                                                                                                                          \n",
    "                self.biases = [b - lr * nb for b, nb in zip(self.biases, delta_nabla_b)]                                                                                                                            \n",
    "            print(n)                                                                                                                                                                                                \n",
    "            # 更新根据累加的偏导值更新 w 和 b，                       (这里因为用了小样本，所以 eta 要除于小样本的长度)                                                                                                                                \n",
    "                                                                                                                                                                                                                    \n",
    "                                                                                                                                                                                                                    \n",
    "            # 每训练200次预测，输出一次预测准确率                                                                                                                                                                                   \n",
    "            if n % 10 == 0 and n != 0:                                                                                                                                                                              \n",
    "                predictions = []                                                                                                                                                                                    \n",
    "                for j in range(X_test.shape[0]):                                                                                                                                                                    \n",
    "                    # 获取预测结果：返回与十个标签值逼近的距离，数值最大的选为本次的预测值                                                                                                                                                            \n",
    "                    output = self.predict(X_test[j])                                                                                                                                                                \n",
    "                    # 将最大的数值所对应的输出层神经元记为1，其余输出层神经元为0                                                                                                                                                                \n",
    "                    predictions.append(np.argmax(output))  # 获取预测结果                                                                                                                                                                                                                                                                              \n",
    "                #print(y_test)                                                                                                                                                                                      \n",
    "                acc = np.mean(np.equal(predictions, y_test))                                                                                                                                                        \n",
    "                #print(len(predictions))                                                                                                                                                                            \n",
    "                accuracy.append(acc)                                                                                                                                                                                \n",
    "                print(acc)                                                                                                                                                                                          \n",
    "                cost = np.mean(np.square(y_test - predictions) / 2)                                                                                                                                                                    \n",
    "                #print(cost)                                                                                                                                                                                        \n",
    "                loss.append(cost)                                                                                                                                                                                   \n",
    "                # np.equal()：相同返回true，不同返回false                                                                                                                                                                     \n",
    "        return accuracy, loss                                                                                                                                                                                       \n",
    "                                                                                                                                                                                                                    \n",
    "    def predict(self, x):                                                                                                                                                                                           \n",
    "        # 计算输出层得到的结果                                                                                                                                                                                                \n",
    "        # argmax()返回的就是最                                                                                                                                                                                            \n",
    "        activation = x                                                                                                                                                                                              \n",
    "        for b, w in zip(self.biases, self.weights):                                                                                                                                                                 \n",
    "            if b.ndim == 1:                                                                                                                                                                                         \n",
    "                b = b.reshape(1, len(b))                                                                                                                                                                            \n",
    "            if activation.ndim == 1:                                                                                                                                                                                \n",
    "                activation = activation.reshape(1, len(activation))                                                                                                                                                 \n",
    "            if w.shape[1] != activation.shape[0]:                                                                                                                                                                   \n",
    "                activation = activation.transpose()                                                                                                                                                                 \n",
    "            dot_res = np.dot(w, activation)                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                    \n",
    "            layer2 = dot_res + b                                                                                                                                                                                    \n",
    "            activation = sigmoid(layer2)                                                                                                                                                                            \n",
    "        #print(activation)                                                                                                                                                                                          \n",
    "        return activation                                                                                                                                                                                           \n",
    "                                                                                                                                                                                                                                                                                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a336a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "0.7911111111111111\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "0.8022222222222222\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "0.8044444444444444\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "0.9044444444444445\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "0.9022222222222223\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "0.9044444444444445\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "0.9044444444444445\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "0.9044444444444445\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "0.9044444444444445\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "0.9044444444444445\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "0.9044444444444445\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "0.9044444444444445\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "0.9044444444444445\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "0.9044444444444445\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "0.9044444444444445\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "0.9044444444444445\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "0.9044444444444445\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "0.9044444444444445\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "0.9044444444444445\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "0.9044444444444445\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "0.9044444444444445\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "0.9044444444444445\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "0.9044444444444445\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "0.9044444444444445\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "0.9044444444444445\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "0.9044444444444445\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "0.9066666666666666\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "0.9888888888888889\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "0.9888888888888889\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "0.9911111111111112\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "0.9911111111111112\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "0.9911111111111112\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "0.9888888888888889\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "0.9888888888888889\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "0.9888888888888889\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "0.9888888888888889\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "0.9888888888888889\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "0.9888888888888889\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "0.9888888888888889\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "0.9888888888888889\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "0.9888888888888889\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "0.9888888888888889\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "0.9888888888888889\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "0.9888888888888889\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "0.9888888888888889\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "0.9888888888888889\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "0.9888888888888889\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "0.9888888888888889\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "0.9888888888888889\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "0.9888888888888889\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "0.9888888888888889\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "0.9888888888888889\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "0.9888888888888889\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "0.9888888888888889\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "0.9888888888888889\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "0.9888888888888889\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "0.9888888888888889\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "0.9888888888888889\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "0.9888888888888889\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "0.9888888888888889\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "0.9888888888888889\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "0.9888888888888889\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "0.9888888888888889\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "0.9888888888888889\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "0.9888888888888889\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "0.9888888888888889\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "0.9888888888888889\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "0.9888888888888889\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "0.9888888888888889\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "0.9888888888888889\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "0.9888888888888889\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "0.9888888888888889\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "0.9888888888888889\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "0.9888888888888889\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "0.9888888888888889\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "0.9888888888888889\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "0.9888888888888889\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "0.9888888888888889\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "0.9888888888888889\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "0.9888888888888889\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "0.9888888888888889\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "0.9888888888888889\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "0.9888888888888889\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "0.9888888888888889\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "0.9888888888888889\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "0.9888888888888889\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "0.9888888888888889\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "0.9888888888888889\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "0.9888888888888889\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "0.9888888888888889\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "0.9888888888888889\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "0.9888888888888889\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "0.9888888888888889\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "0.9888888888888889\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "0.9888888888888889\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "0.9888888888888889\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "0.9888888888888889\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "0.9888888888888889\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "0.9888888888888889\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([64,100,10])#创建网络\n",
    " \n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y) #分割数据\n",
    "\n",
    "labels_train = LabelBinarizer().fit_transform(y_train)#标签二值化\n",
    "labels_test = LabelBinarizer().fit_transform(y_test)#标签二值化\n",
    "\n",
    "epoch = 1000\n",
    "accuracy, loss = nn.train(X_train,labels_train,epochs=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82efbbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练结果可视化\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(0, epoch, int(epoch/len(loss))), loss)\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(0, epoch, int(epoch/(len(accuracy)))), accuracy)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c449c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
