import java.sql.Timestamp
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

// Create DataFrame representing the stream of input lines from connection to host:port
val inputLines = spark.readStream.format("socket").option("host", "localhost").option("port", 9999).option("includeTimestamp", true).load()

// Split the lines into words, retaining timestamps
val words = inputLines.as[(String, Timestamp)].flatMap(line => line._1.split(" ").map(word => (word, line._2))).toDF("word", "timestamp")

// Group the data by window and word and compute the count of each group
val windowedCounts = words.withWatermark("timestamp", "10 seconds").groupBy( window($"timestamp", "10 seconds", "10 seconds"), $"word").count().orderBy("window")

// Start running the query that prints the windowed word counts to the console
val query = windowedCounts.writeStream.outputMode("complete").format("console").option("truncate", "false")

query.start()
query.awaitTermination()

